{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8]) tensor([[[ 14.4183,   4.0192,   4.9964,  -9.4050,   7.4996,  -1.2957,  10.1465,\n",
      "           -7.6784],\n",
      "         [-11.7635,   7.4509,   0.2277, -16.4446,  -4.2662,   0.7789,   7.5461,\n",
      "           -3.1969],\n",
      "         [ -6.7492,   4.5716,   7.7062,  -4.8321,   3.8501,  -6.5702, -14.2348,\n",
      "            5.2437],\n",
      "         [  0.8196,   9.2905,  -2.3660,   4.6366,  -5.8006,   4.6970,   0.5125,\n",
      "           -4.0809]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 第一种方式，直接图像分块：\n",
    "def image2emb_naive(image,patch_size,weight):\n",
    "    #unfold 函数的返回值[N,C*W*H,L]\n",
    "    patch = F.unfold(image,kernel_size=patch_size,stride=patch_size).transpose(-1,-2)\n",
    "    #图像拆分成patch之后，和权重矩阵相乘，就得到了每个batch Embedding之后的维度：\n",
    "    patch_embedding = torch.matmul(patch,weight)\n",
    "\n",
    "    return patch_embedding\n",
    "\n",
    "bs,ic,image_h,image_w = 1,3,8,8\n",
    "patch_size = 4\n",
    "model_dim = 8 #目标要输出的通道数目；\n",
    "patch_depth = patch_size*patch_size*ic\n",
    "image = torch.randn(bs,ic,image_h,image_w)\n",
    "weight = torch.randn(patch_depth,model_dim)\n",
    "\n",
    "patch_embedding_navie = image2emb_naive(image,patch_size,weight)\n",
    "print(patch_embedding_navie.shape,patch_embedding_navie)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8]) tensor([[[ 14.4183,   4.0192,   4.9964,  -9.4050,   7.4996,  -1.2957,  10.1465,\n",
      "           -7.6784],\n",
      "         [-11.7635,   7.4509,   0.2277, -16.4446,  -4.2662,   0.7789,   7.5461,\n",
      "           -3.1969],\n",
      "         [ -6.7492,   4.5716,   7.7062,  -4.8321,   3.8501,  -6.5702, -14.2348,\n",
      "            5.2437],\n",
      "         [  0.8196,   9.2905,  -2.3660,   4.6366,  -5.8006,   4.6970,   0.5125,\n",
      "           -4.0809]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 第二种方式，卷积的方式：\n",
    "def image2emb_conv(image,kernel,stride):\n",
    "    conv_output = F.conv2d(image,kernel,stride=stride)\n",
    "    bs,oc,ow,oh = conv_output.shape\n",
    "\n",
    "    # 还是那个事，transfomer的输入形状是 NLC，L在图像中就是 W*H ，也就是序列长度；\n",
    "    patch_embedding = conv_output.reshape((bs,oc,ow*oh)).transpose(-1,-2)\n",
    "    return patch_embedding\n",
    "bs,ic,image_h,image_w = 1,3,8,8\n",
    "patch_size = 4\n",
    "model_dim = 8 #目标要输出的通道数目；\n",
    "patch_depth = patch_size*patch_size*ic\n",
    "# image = torch.randn(bs,ic,image_h,image_w)\n",
    "# weight = torch.randn(patch_depth,model_dim)\n",
    "\n",
    "kernle = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size)) #kernel要满足 oc*ic*kh*kw\n",
    "patch_embedding_conv = image2emb_conv(image,kernle,patch_size)\n",
    "print(patch_embedding_conv.shape,patch_embedding_conv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------\n",
    "# 上面完成了图片的embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8]) tensor([[[ 14.4183,   4.0192,   4.9964,  -9.4050,   7.4996,  -1.2957,  10.1465,\n",
      "           -7.6784],\n",
      "         [-11.7635,   7.4509,   0.2277, -16.4446,  -4.2662,   0.7789,   7.5461,\n",
      "           -3.1969],\n",
      "         [ -6.7492,   4.5716,   7.7062,  -4.8321,   3.8501,  -6.5702, -14.2348,\n",
      "            5.2437],\n",
      "         [  0.8196,   9.2905,  -2.3660,   4.6366,  -5.8006,   4.6970,   0.5125,\n",
      "           -4.0809]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 第二种方式，卷积的方式：\n",
    "def image2emb_conv(image,kernel,stride):\n",
    "    conv_output = F.conv2d(image,kernel,stride=stride)\n",
    "    bs,oc,ow,oh = conv_output.shape\n",
    "\n",
    "    # 还是那个事，transfomer的输入形状是 NLC，L在图像中就是 W*H ，也就是序列长度；\n",
    "    patch_embedding = conv_output.reshape((bs,oc,ow*oh)).transpose(-1,-2)\n",
    "    return patch_embedding\n",
    "bs,ic,image_h,image_w = 1,3,8,8\n",
    "patch_size = 4\n",
    "model_dim = 8 #目标要输出的通道数目；\n",
    "patch_depth = patch_size*patch_size*ic\n",
    "max_num_token = 16 #假设最大只能输入16个切割好的patch图像\n",
    "num_classes = 10 # 假设有十个分类别\n",
    "# image = torch.randn(bs,ic,image_h,image_w)\n",
    "# weight = torch.randn(patch_depth,model_dim)\n",
    "\n",
    "kernle = weight.transpose(0,1).reshape((-1,ic,patch_size,patch_size)) #kernel要满足 oc*ic*kh*kw\n",
    "patch_embedding_conv = image2emb_conv(image,kernle,patch_size)\n",
    "print(patch_embedding_conv.shape,patch_embedding_conv)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1155, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------------------------------\n",
    "# cls_token 部分：\n",
    "cls_token_embedding = torch.randn(bs,1,model_dim,requires_grad=True)\n",
    "token_embedding = torch.cat([cls_token_embedding,patch_embedding_conv],dim=1)\n",
    "\n",
    "# 位置编码：\n",
    "position_embedding_table = torch.randn(max_num_token,model_dim,requires_grad=True)\n",
    "segment_length = token_embedding.shape[1]\n",
    "\n",
    "# 将位置embedding复制bs份\n",
    "position_embedding = torch.tile(position_embedding_table[:segment_length],[token_embedding.shape[0],1,1])\n",
    "token_embedding += position_embedding\n",
    "\n",
    "#调用包\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim,nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\n",
    "encoder_output = transformer_encoder(token_embedding)\n",
    "\n",
    "# 分类\n",
    "cls_token_output = encoder_output[:,0,:] #只取输出的第一个 序列，这个就是cls token\n",
    "\n",
    "linear_layer = nn.Linear(model_dim,num_classes)\n",
    "\n",
    "logits = linear_layer(cls_token_output)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "label = torch.randint(10,(bs,))\n",
    "\n",
    "loss = loss_fn(logits,label)\n",
    "print(loss)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
